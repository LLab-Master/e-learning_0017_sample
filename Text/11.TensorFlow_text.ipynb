{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow を使ったテキストデータの分類"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ニュース記事の分類器を作る\n",
    "　学習データはLivedoorニュースコーパスを用います。\n",
    "\n",
    "　Livedoor ニュースコーパスとは\n",
    "- 本コーパスは、NHN Japan株式会社が運営する「Livedoor ニュース」のうち、下記のクリエイティブ・コモンズライセンスが適用されるニュース記事を収集し、可能な限りHTMLタグを取り除いて作成したものです。\n",
    "- 収集時期は2012年9月上旬\n",
    "- 収集されているデータは、\n",
    "  - ニューストピック\n",
    "  - Sports Watch\n",
    "  - ITライフハック\n",
    "  - 家電チャンネル\n",
    "  - MOVIE ENTER\n",
    "  - 独女通信\n",
    "  - エスマックス\n",
    "  - Livedoor HOME\n",
    "  - Peachy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの読み込み\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import tarfile\n",
    "\n",
    "url = 'http://www.rondhuit.com/download/ldcc-20140209.tar.gz'\n",
    "res = requests.get(url,stream=True)\n",
    "if res.status_code == 200:\n",
    "    with open('ldcc-20140209.tar.gz', 'wb') as file:\n",
    "            for chunk in res.iter_content(chunk_size=1024):\n",
    "                file.write(chunk)\n",
    "\n",
    "\n",
    "tf = tarfile.open('ldcc-20140209.tar.gz', 'r')\n",
    "tf.extractall('testdata')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テキストデータのベクトル化\n",
    "- テキストデータを数値化する代表的な手法\n",
    "  - BoW　\n",
    "  - TF-IDF\n",
    "\n",
    "### BoW\n",
    "- Bag of Words の略。単純にテキストデータに含まれる単語の数を数える手法。単純な手法なのでプログラムは組みやすい。\n",
    "- scikit-learnではBoWを実現するためのモジュールが提供されていおり、簡単に試せる。\n",
    "- 例「This is a pen. I have a pen.」\n",
    "\n",
    "　　　　<img src=\"fig/bow.png\" width=\"20%\">\n",
    "\n",
    "### BoW の実装\n",
    "- Scikit-learnのCountVectorizerを使用する。\n",
    "- 文章のデータ（リスト）を用意する。\n",
    "- fit_transform() メソッドで単語を数える。\n",
    "- fit_transform()の結果は単語を数えた結果の２次元配列が返される。\n",
    "- get_feature_names（）で出現単語の一覧を確認できる。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['do' 'go' 'hard' 'in' 'is' 'it' 'not' 'out' 'rain' 'raining' 'shining'\n",
      " 'sun' 'the']\n",
      "[[0 0 0 0 1 0 0 0 0 0 1 1 1]\n",
      " [0 0 1 0 1 1 0 0 0 1 0 0 0]\n",
      " [1 1 0 1 0 0 1 1 1 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vec = CountVectorizer()\n",
    "docs = [ 'The sun is shining',\n",
    "         'It is raining hard',\n",
    "         'Do not go out in the rain']\n",
    "\n",
    "bag = count_vec.fit_transform(docs)\n",
    "\n",
    "print(count_vec.get_feature_names_out())\n",
    "\n",
    "print(bag.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 日本語の特徴\n",
    "- 英語は単語と単語の間にスペース（空白）が入るので単語の分解が容易。日本語は単語と単語の間にスペースがない言語である。\n",
    "- 単語に分解することを「分かち書き」と言う。日本語はどうように「分かち書き」をするかが重要である。\n",
    "- 日本語は「分かち書き」するために文章を意味のある単語(形態素）まで分解する技術が必要。この技術を「形態素解析」という。\n",
    "- 形態素解析のアルゴリズムは色々研究されているが、日本語のあいまいさにより完璧に「分かち書き」するのはとても難しい。\n",
    "- 例えば次の文章は2通りの解釈ができる。\n",
    "\n",
    "　「かれのくるまでまつ」\n",
    "\n",
    "　「かれ（彼）　の　くる(来る）まで　まつ（待つ）」\n",
    "\n",
    "　「かれ（彼）　の　くるま（車）　で　待つ（待つ）」\n",
    "\n",
    "### Python形態素解析ライブラリ\n",
    "- Pythonでは日本語形態素解析用のライブラリは「MeCab」と「janome」が有名である。\n",
    "  - 「MeCab」はインストールに手間が掛かる。\n",
    "  - 「janome」はインストールが簡単で容易に試せるので、「janome」を利用。\n",
    "\n",
    "- テキストデータのクレンジング\n",
    "  - 名詞のみを抽出対象とする\n",
    "  - 出現頻度の低い単語、逆に出現頻度が多すぎる単語は除外する。\n",
    "  - ベクトル化するコードの実行は非常に時間がかかる。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ベクトル化の実装\n",
    "#### ラベル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_NAME = ['dokujo-tsushin', 'it-life-hack','kaden-channel', \n",
    "             'livedoor-homme', 'movie-enter', 'peachy','smax',\n",
    "             'sports-watch', 'topic-news']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### テキストデータの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#データを読む\n",
    "import os\n",
    "\n",
    "news_dir='testdata/text'\n",
    "excluded = ['CHANGES.txt','README.txt','LICENSE.txt']\n",
    "docs ,labels, = [],[]\n",
    "fnames = []\n",
    "\n",
    "for label_no, label in enumerate(LABEL_NAME):\n",
    "    dir = news_dir + '/' + label\n",
    "    files = os.listdir(dir)\n",
    "    for f in files:\n",
    "        if f in excluded:\n",
    "            continue\n",
    "        fname = dir + '/' + f\n",
    "        if not os.path.isfile(fname):\n",
    "            continue\n",
    "        with open(fname,'r') as f:\n",
    "            try:\n",
    "                data = ''.join(f.readlines()[2:])  # 1〜2行目は捨てる\n",
    "                docs.append(data)\n",
    "                labels.append(label_no)\n",
    "                fnames.append(fname)\n",
    "            except KeyError:\n",
    "                pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 読み込んだデータをPandasで扱う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "#Pandasで管理\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.DataFrame([labels,docs,fnames])\n",
    "df = df.T\n",
    "#data = df.reindex(np.random.permutation(df.index)).reset_index(drop=True)\n",
    "data = df\n",
    "data.columns=['L','T','F']\n",
    "data.loc[ data['L'] == 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(data['L'])\n",
    "docs_r = np.array(data['T'])\n",
    "file_names = np.array(data['F'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### テキストデータをベクタライズ（参考）\n",
    "　処理に時間がかかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "def extract_words(text):\n",
    "    t = Tokenizer()\n",
    "    token = t.tokenize(text)\n",
    "    words = []\n",
    "    for word in token:\n",
    "        if word.part_of_speech.find('名詞') >= 0 :\n",
    "            if not word.surface.isdigit():\n",
    "                words.append(word.surface)\n",
    "        \n",
    "    return words\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer=extract_words,min_df=20,max_df=0.3)\n",
    "bag = vectorizer.fit_transform(docs_r[:])\n",
    "X = bag.toarray()\n",
    "y_t = labels[:]\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ベクタライズしたデータの保存（参考）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存\n",
    "import numpy as np\n",
    "\n",
    "np.savez('vectorize_result_all3',X)\n",
    "np.save('labels3',y_t)\n",
    "np.save('fnames3', file_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ベクタライズした結果をロードする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7020\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "Z = np.load('vectorize_result_all3.npz',allow_pickle=True)\n",
    "X = Z['arr_0']\n",
    "print(len(X[0]))\n",
    "labels = np.load('labels3.npy',allow_pickle=True)\n",
    "fnames = np.load('fnames3.npy',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 次元削減\n",
    "### 次元削減とは\n",
    "学習データの次元数が多いと組み合わせが膨大になり機械学習の計算が効率良く行えなくなる。\n",
    "\n",
    "これを「次元の呪い」と呼ばれている。\n",
    "\n",
    "データの特徴を損なわずに次元を削減する手法を次元削減と言う。\n",
    "\n",
    "### 次元削減の主な手法\n",
    "- 主成分分析(PCA)\n",
    "- 特異値分解(SVD)\n",
    "\n",
    "いずれもscikit-learnでサポートされているアルゴリズム。今回は主成分分析を使う。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 7020次元を300次元に次元削減"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=300)\n",
    "X = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正解ラベルのOneHoneベクトル化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "train_t = labels.reshape([len(X), 1])\n",
    "\n",
    "# 正解ラベルの　OneHot化\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# encode label\n",
    "# encoder = OneHotEncoder(n_values=max(train_t)+1)\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "train_y= encoder.fit_transform(train_t).toarray()\n",
    "print(train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 計算モデル\n",
    "　以下のようなモデルを考える。\n",
    "\n",
    "　　　　<img src=\"fig/text_model.png\" width=\"50%\">\n",
    "\n",
    "　　　"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kerasモデル定義\n",
    "\n",
    "(Kerasでの実装は演習にて実施します)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(G)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "11.2667px",
    "width": "251.267px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "a11ed55f7b970296d635ebe63e6fde2301be6254f902f31dbdda195d20319d54"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
